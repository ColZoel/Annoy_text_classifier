@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention Is All You Need},
  journal      = {CoRR},
  volume       = {abs/1706.03762},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.03762},
  eprinttype    = {arXiv},
  eprint       = {1706.03762},
  timestamp    = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{minilm,
  author       = {Wenhui Wang and
                  Furu Wei and
                  Li Dong and
                  Hangbo Bao and
                  Nan Yang and
                  Ming Zhou},
  title        = {MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression
                  of Pre-Trained Transformers},
  journal      = {CoRR},
  volume       = {abs/2002.10957},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.10957},
  eprinttype    = {arXiv},
  eprint       = {2002.10957},
  timestamp    = {Fri, 19 Apr 2024 15:54:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-10957.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{ANNOYBern,
  author = {Bernhardsson, Erik},
  title = {Nearest neighbors and vector models, part 2: How to search in high-dimensional spaces},
  year = {2015},
  month = {10},
  url = {https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html},
  note = {Blog post}
}


@article{transformer-text-class,
title = {Improving text classification with transformers and layer normalization},
journal = {Machine Learning with Applications},
volume = {10},
pages = {100403},
year = {2022},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2022.100403},
url = {https://www.sciencedirect.com/science/article/pii/S2666827022000792},
author = {Ben Rodrawangpai and Witawat Daungjaiboon},
keywords = {Computational modeling, Machine learning, Natural language processing, Pattern classification, Semantics, Text analysis, Transformers},
abstract = {More than 25,000 injuries and 25 fatalities occur each year due to unstable furniture tip-over incidents. Classifying these furniture tip-over incidents is an essential task for understanding incident patterns and building safer products. For example, this classification can help standards development organizations (SDOs) and policy makers discover hidden insights, which can be used to develop standards and regulations that help improve furniture and make homes safer. Since 2000, the U.S. Consumer Product Safety Commission (CPSC) has published data related to consumer product injuries. The amount of data has grown rapidly, and the process of manually reviewing and classifying individual incidents has correspondingly become very resource intensive. This paper proposes an improved method that employs a combination of natural language processing (NLP) techniques and machine learning (ML) algorithms to classify textual data. Machine learning models can help reduce time and effort by streamlining incident narrative classification for determining whether incidents are related to furniture tip-overs. Challenges often presented by real-world data sets (such as the CPSC data used in our experiment) include imbalanced target classes and narratives requiring domain knowledge, since the data sets contain abbreviations and jargon. Using out-of-the-box, default classification models such as bidirectional encoder representations from transformers (BERT) might not yield adequate results. Our proposed method adds layer normalization and dropout layers to a transformer-based language model, which achieves better classification results than using a transformer-based language alone with imbalanced classes. We carefully measure the impact of hidden layers in order to fine-tune the model.}
}

@article{lu2022comparative,
  title={A comparative study on deep learning models for text classification of unstructured medical notes with various levels of class imbalance},
  author={Lu, Hongxia and Ehwerhemuepha, Louis and Rakovski, Cyril},
  journal={BMC medical research methodology},
  volume={22},
  number={1},
  pages={181},
  year={2022},
  publisher={Springer}
}

@INPROCEEDINGS{gokhan-trans-text-class,
  author={Soyalp, Gokhan and Alar, Artun and Ozkanli, Kaan and Yildiz, Beytullah},
  booktitle={2021 6th International Conference on Computer Science and Engineering (UBMK)}, 
  title={Improving Text Classification with Transformer}, 
  year={2021},
  volume={},
  number={},
  pages={707-712},
  keywords={Training;Computational modeling;Text categorization;Tools;Transformers;Data models;Natural language processing;Deep Learning;Natural Language Processing;Text Classification;Transformer;Attention;Word Embedding},
  doi={10.1109/UBMK52708.2021.9558906}}

@article{DINGEL2020,
title = {How many jobs can be done at home?},
journal = {Journal of Public Economics},
volume = {189},
pages = {104235},
year = {2020},
issn = {0047-2727},
doi = {https://doi.org/10.1016/j.jpubeco.2020.104235},
url = {https://www.sciencedirect.com/science/article/pii/S0047272720300992},
author = {Jonathan I. Dingel and Brent Neiman},
keywords = {Remote work, Telecommuting, Work from home},

}

@article{bergstra12a_randomSearch,
  author  = {James Bergstra and Yoshua Bengio},
  title   = {Random Search for Hyper-Parameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {10},
  pages   = {281--305},
  url     = {http://jmlr.org/papers/v13/bergstra12a.html}
}

@misc{huggingface,
  author = {Hugging Face},
  title = {How do Transformers work?},
  year = {2021},
  url = {https://huggingface.co/learn/nlp-course/en/chapter1/4}
}

@misc{spacy,
author = {SpaCy},
title = {Large Language Models},
URL = {https://spacy.io/api/large-language-models}
}