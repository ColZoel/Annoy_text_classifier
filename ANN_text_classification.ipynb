{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Approximate Nearest Neighbors for Text Classification\n",
    "Author: [Collin Zoeller](https://www.linkedin.com/in/collinzoeller)\n",
    "<br> Carnegie Mellon University\n",
    "\n",
    "This notebook demonstrates how to use the ANNOY library for fast approximate nearest neighbor search to classify text data. The goal is to classify user-generated text data into pre-defined categories using a pre-trained transformer model. The ANNOY library is used to build an approximate nearest neighbor index for the target classes, and then to classify new observations based on their nearest neighbors in the embedding space.\n"
   ],
   "id": "2e8cbd05f4483d2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Imports",
   "id": "6907ae99e42d90c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install -q sentence-transformers\n",
    "!pip install -q annoy\n",
    "!pip install -q kagglehub"
   ],
   "id": "9b43540e75b516a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T19:34:07.905440Z",
     "start_time": "2025-03-22T19:33:53.749965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from annoy import AnnoyIndex\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from typing import Union, Self\n"
   ],
   "id": "40ad25b6946a9cce",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/collinzoeller/anaconda3/envs/AnnoyClassifier/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "\n",
    "Use 100k samples of previously scraped data to train and evaluate the model. produce Synthetic data by adding noise to the scraped data.\n",
    "\n",
    "\n",
    "REDDIT DATA\n",
    "- Data retrieved from: https://www.reddit.com/r/datasets/comments/w340kj/dataset_of_job_descriptions_for_your_pleasure/\n",
    "- Data hosted at: https://drive.google.com/drive/folders/1XxNuhiei5taFR6gziofYAx0oWfGeV7y9\n",
    "\n",
    "KAGGLE DATA\n",
    "- SOURCE: https://www.kaggle.com/datasets/jatinchawda/job-titles-and-description\n"
   ],
   "id": "9bfb4c4dc0e04433"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_training_data(num_obs=None):\n",
    "    \"\"\"Create training data from scraped data.\"\"\"\n",
    "    df = pd.read_parquet('data/kaggle_clean_data.parquet')\n",
    "    df = df.rename(columns={\"job_title\": \"title\"})\n",
    "    df1 = pd.read_parquet('data/reddit_data.parquet')\n",
    "    df = pd.concat([df, df1])\n",
    "    df = clean_data(df)\n",
    "\n",
    "    if num_obs:\n",
    "        df = df.sample(num_obs)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_data(data:np.array):\n",
    "    \"\"\"\n",
    "    Standardize data for english ASCII-only characters.\n",
    "    :param data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data, columns=['X'])\n",
    "\n",
    "   # non-ascii\n",
    "    df['X'] = df['X'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "\n",
    "    # remove empty strings\n",
    "    df = df[df['X'] != \"\"]\n",
    "\n",
    "    return df.X.to_numpy()\n",
    "\n",
    "\n",
    "def noisify(truevals: np.array):\n",
    "    \"\"\"Add noise to the data.\"\"\"\n",
    "\n",
    "    df = pd.DataFrame(truevals, columns=['X'])\n",
    "    if random.random() < 0.3:\n",
    "        df['X'] = df['X'].str.upper()\n",
    "    if random.random() > 0.5:\n",
    "        df['X'] = df['X'].str.lower()\n",
    "    if random.random() < 0.5:\n",
    "        df['X'] = df['X'].apply(lambda x: x + \" \" + random.choice([\"Senior\", \"Junior\", \"Lead\"]))\n",
    "    if random.random() < 0.3:\n",
    "        df['X'] = df['X'].apply(lambda x: \"\".join(list(x).pop(random.randint(0, len(x) - 1))))\n",
    "    if random.random() < 0.1:\n",
    "        df['X'] = df['X'].str[::-1]\n",
    "    if random.random() < 0.2:\n",
    "        df['X'] = df['X'].apply(lambda x: x.replace(\" \", random.choice([\"_\", \"-\", \"\"])))\n",
    "    if random.random() < 0.2:\n",
    "        df['X'] = df['X'].apply(lambda x: x + str(random.randint(0, 99)))\n",
    "    if random.random() < 0.2:\n",
    "        df['X'] = df['X'].apply(lambda x: x[:random.randint(1, len(x))])\n",
    "    return df.X.to_numpy()\n",
    "\n",
    "\n",
    "def make_random_data(classes, num_obs=1000, colidx=0):\n",
    "    \"\"\"Create random data with noise.\"\"\"\n",
    "    if isinstance(classes, str) and classes.endswith('.csv'):\n",
    "        df = pd.read_csv(classes)\n",
    "        classes = df.iloc[:, colidx].unique()\n",
    "\n",
    "    true_values = np.random.choice(classes, num_obs)\n",
    "    noisy_data = noisify(true_values)\n",
    "    return true_values, noisy_data"
   ],
   "id": "d1dd9ddc2a433dd1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Download the Kaggle dataset",
   "id": "bb02cb78c80364cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if not os.path.exists(\"data/kaggle_clean_data.parquet\"):\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    path = kagglehub.dataset_download(\"jatinchawda/job-titles-and-description\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    shutil.move(f\"{path}/clean_data.parquet\", \"data/kaggle_clean_data.parquet\")\n",
    "\n",
    "    # Save only the title column\n",
    "    df = pd.read_parquet(\"data/kaggle_clean_data.parquet\", columns=[\"job_title\"])\n",
    "    df.to_parquet(\"data/kaggle_clean_data.parquet\")\n",
    "\n",
    "else:\n",
    "    print(\"Kaggle data already downloaded.\")"
   ],
   "id": "4be49ec29d468810"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Format the Reddit dataset\n",
    "This data should already be downloaded from https://drive.google.com/drive/folders/1XxNuhiei5taFR6gziofYAx0oWfGeV7y9 and saved as data/reddit_jobs."
   ],
   "id": "1e6068258b4a5f93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "if not os.path.exists(\"data/reddit_data.parquet\"):\n",
    "\n",
    "    if not os.path.exists(\"data/reddit_jobs\"):\n",
    "        raise FileNotFoundError(\"Download the Reddit dataset from the drive at\"\n",
    "                                \" https://drive.google.com/drive/folders/1XxNuhiei5taFR6gziofYAx0oWfGeV7y9 .\"\n",
    "                                \"\\nSave as data/reddit_jobs.\")\n",
    "\n",
    "    print(\"Formatting Reddit data...\")\n",
    "    df = pd.concat([pd.read_csv(file) for file in glob(f\"data/reddit_jobs/*.csv\")])\n",
    "    df=df['title'].to_frame()\n",
    "    df.to_parquet(\"data/reddit_data.parquet\")\n",
    "    print(\"Reddit data saved to data/reddit_data.parquet\")\n",
    "else:\n",
    "    print(\"Reddit data already formatted correctly.\")"
   ],
   "id": "949203c4e111f5a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "15d7cdd45d872b30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modules\n",
    "\n",
    "Functions and tools. The process is modularized for simplicity.\n"
   ],
   "id": "25f0dcd99ae22bb7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions and tools for the occupation classification project.\n",
    "\"\"\"\n",
    "\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"(finished in {time.time() - start:.2f} seconds, \"\n",
    "              f\"avg: {(time.time() - start) / len(args[1]):.4f} sec/label)\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "    \"\"\"\n",
    "    Encoder for the given model. The model can be a string or a SentenceTransformer object.\n",
    "    Embeddings can be saved to or loaded from disk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: Union[os.PathLike, str, SentenceTransformer]):\n",
    "\n",
    "        \"\"\"\n",
    "        :param model: path, name, SentenceTransformer object, or TransformerLoader object\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = SentenceTransformer(model) if not isinstance(model, SentenceTransformer) else model\n",
    "        self.embeds = None\n",
    "        self.labels = None\n",
    "\n",
    "\n",
    "    @timer\n",
    "    def embed(self, data: np.array, batched: bool = False, batch_size: int = 1000) -> Self:\n",
    "\n",
    "        \"\"\"\n",
    "        Embed the given data using the model.\n",
    "        :param data: Array of string data to be embedded\n",
    "        :param batched: Whether to use batched encoding, better for large datasets\n",
    "        :param batch_size: Batch size for batched encoding (irrelevant if batched is False)\n",
    "        :return: Encoder object\n",
    "        \"\"\"\n",
    "\n",
    "        self.labels = data\n",
    "\n",
    "        if not batched:\n",
    "            print(f\"\\nEncoding {len(data)} target label values\")\n",
    "            self.embeds = self.model.encode(data, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "            return self\n",
    "\n",
    "        else:\n",
    "            num_batches = (len(data) + batch_size - 1) // batch_size\n",
    "            print(f\"\\nEncoding {len(data)} target label values ({num_batches} batches)\")\n",
    "            batch_indices = np.array_split(np.arange(len(data)), num_batches)\n",
    "\n",
    "            self.embeds = np.vstack([self.model.encode(data[indices.tolist()],\n",
    "                                                       convert_to_numpy=True,\n",
    "                                                       show_progress_bar=False) for indices in batch_indices])\n",
    "            return self\n",
    "\n",
    "    def save(self, path: Union[os.PathLike, str]) -> np.array:\n",
    "        np.savez_compressed(path, labels=self.labels, embeds=self.embeds)\n",
    "        return self.labels, self.embeds\n",
    "\n",
    "    def load(self, path: str) -> np.array:\n",
    "        data = np.load(path)\n",
    "        self.labels = data['arr_0']\n",
    "        self.embeds = data['arr_1']\n",
    "        self.embeds = np.load(path)\n",
    "        return self.labels, self.embeds\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "\n",
    "    \"\"\"\n",
    "    Classifier using Annoy index for fast nearest neighbor search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "        self.eval = {}\n",
    "        self.x_embeddings = None\n",
    "        self.x_labels = None\n",
    "        self.pred = None\n",
    "        self.y_embeddings = None\n",
    "        self.y_labels = None\n",
    "\n",
    "    def build_tree(self, labels: np.array, embeddings: np.array, num_trees=10):\n",
    "\n",
    "        \"\"\"Build an Annoy index for the given embeddings.\n",
    "        labels should be same size as embeddings.\n",
    "        :param labels: string labels\n",
    "        :param embeddings: Embeddings to build the index\n",
    "        :param num_trees: Number of trees to build\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"\\nBuilding Annoy index with {num_trees} trees\")\n",
    "        t = AnnoyIndex(embeddings.shape[1], 'euclidean')\n",
    "        for i, emb in enumerate(embeddings):\n",
    "            t.add_item(i, emb)\n",
    "        t.build(num_trees)\n",
    "        self.tree = t\n",
    "\n",
    "        self.y_embeddings = embeddings\n",
    "        self.y_labels = labels\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self,\n",
    "                labels: np.array,\n",
    "                embeddings: np.array,\n",
    "                neighbors: int = 1,\n",
    "                save_path: Union[os.PathLike, str, None] = None) -> Self:\n",
    "\n",
    "        \"\"\"\n",
    "        Predict the labels for the given embeddings using the Annoy index.\n",
    "        :param labels: labels for the embeddings\n",
    "        :param embeddings: numerical representation of the labels (should be same length as labels)\n",
    "        :param neighbors: number of neighbors to consider\n",
    "        :param save_path:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"\\nPredicting labels for {len(embeddings)} embeddings\")\n",
    "        indices = [self.tree.get_nns_by_vector(emb.tolist(), neighbors) for emb in embeddings]\n",
    "\n",
    "        # Map indices to their corresponding class labels\n",
    "        neighbor_labels = [[labels[idx] for idx in idxs] for idxs in indices]\n",
    "\n",
    "        # Determine the most common label among neighbors\n",
    "        y_pred = [Counter(labels).most_common(1)[0][0] for labels in neighbor_labels]\n",
    "\n",
    "        self.x_labels = labels\n",
    "        self.x_embeddings = embeddings\n",
    "        self.pred = y_pred\n",
    "\n",
    "        # Save the predictions\n",
    "        if save_path is not None:\n",
    "            if save_path.endswith(\".csv\"):\n",
    "                df = pd.DataFrame([labels, y_pred],\n",
    "                                  columns=[\"label\", \"yhat \"])\n",
    "                df.to_csv(save_path)\n",
    "            elif save_path.endswith(\".parquet\"):\n",
    "                df = pd.DataFrame([labels, y_pred],\n",
    "                                  columns=[\"label\", \"yhat\"])\n",
    "                df.to_parquet(save_path)\n",
    "\n",
    "            else:\n",
    "                np.savez_compressed(save_path, [labels, y_pred])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def evaluate(self,\n",
    "                 y_true: np.array,\n",
    "                 save_path: Union[os.PathLike, str, None] = None) -> None:\n",
    "\n",
    "        \"\"\"Evaluate the classification performance against an array of ground truth labels.\n",
    "        You must predict the labels of the corresponding text before calling this function.\n",
    "        :param y_true: True labels\n",
    "        :param save_path: Name or path of the report csv\n",
    "        :return: prints overall metrics to console and saves micro/macro metrics to a csv if path is provided\n",
    "        \"\"\"\n",
    "\n",
    "        # macro (overall) metrics\n",
    "        comp = pd.DataFrame({\"label\": y_true, \"yhat\": self.pred})\n",
    "        comp['correct'] = comp['label'] == comp['yhat']\n",
    "        accuracy = comp['correct'].mean()\n",
    "        precision = comp.groupby('yhat')['correct'].mean().mean()\n",
    "        recall = comp.groupby('label')['correct'].mean().mean()\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        macros = {\n",
    "            \"overall\": {\n",
    "                        \"accuracy\": accuracy,\n",
    "                        \"precision\": precision,\n",
    "                        \"recall\": recall,\n",
    "                        \"f1\": f1\n",
    "                        }\n",
    "        }\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\"\n",
    "              f\"\\nPrecision: {precision:.4f}\"\n",
    "              f\"\\nRecall: {recall:.4f}\"\n",
    "              f\"\\nF1: {f1:.4f}\")\n",
    "\n",
    "        # Micro (per-class) metrics\n",
    "        report = classification_report(y_true, self.pred, output_dict=True)\n",
    "        report = {**report, **macros}\n",
    "        self.eval = report\n",
    "        if save_path is not None:\n",
    "            pd.DataFrame(report).T.to_csv(f\"{save_path}.csv\")\n",
    "        return\n",
    "\n",
    "    def visualize(self,\n",
    "                  top: int = 10,\n",
    "                  label_points: bool = False,\n",
    "                  figsize: tuple[int, int] = (10, 10),\n",
    "                  save: Union[os.PathLike, None] = None) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Visualize the embeddings and their labels.\n",
    "        :param top: Number of top classes to plot (>20 classes runs out of colors)\n",
    "        :param label_points: Label the target class points\n",
    "        :param figsize: Figure size (larger size better for large top)\n",
    "        :param save: path to save the plot\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x_embeddings = np.vstack(self.x_embeddings)\n",
    "        x_labels = np.array(self.pred)\n",
    "        y_embeddings = np.vstack(self.y_embeddings)\n",
    "        y_labels = np.array(self.y_labels)\n",
    "\n",
    "        # PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_embed_x = pca.fit_transform(x_embeddings)\n",
    "        pca_embed_y = pca.transform(y_embeddings)\n",
    "\n",
    "        x_unique_classes, x_counts = np.unique(x_labels, return_counts=True)\n",
    "        n_unique_classes = x_unique_classes[np.argsort(-x_counts)[:top]]\n",
    "\n",
    "        # Plot\n",
    "        colors = plt.colormaps['tab20']\n",
    "        class_to_color = {cls: colors(i) for i, cls in enumerate(n_unique_classes)}\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "\n",
    "        # plot input embeddings\n",
    "        for cls in n_unique_classes:\n",
    "            cls_idx = np.where(x_labels == cls)[0]\n",
    "            print(f\"Class: {cls}, Count: {len(cls_idx)}\")\n",
    "            plt.scatter(pca_embed_x[cls_idx, 0], pca_embed_x[cls_idx, 1],\n",
    "                        color=class_to_color[cls], alpha=0.6, marker='o')\n",
    "\n",
    "        # Label classes\n",
    "        for cls in n_unique_classes:\n",
    "            cls_idx = np.where(y_labels == cls)[0]\n",
    "            plt.scatter(pca_embed_y[cls_idx, 0], pca_embed_y[cls_idx, 1],\n",
    "                        color=class_to_color[cls], label=f\"{cls}\", alpha=1.0, marker='x', s=250)\n",
    "            plt.title(f\"Embedding Clusters: Top {top} Occupations\")\n",
    "\n",
    "            if label_points:\n",
    "                for idx in cls_idx:\n",
    "                    plt.text(pca_embed_y[idx, 0], pca_embed_y[idx, 1], cls, fontsize=9, ha='center',\n",
    "                             va='bottom')\n",
    "\n",
    "        if not label_points:\n",
    "            plt.legend(loc='upper right')\n",
    "\n",
    "        plt.show()\n",
    "        if save is not None:\n",
    "            plt.savefig(save)\n",
    "        return\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pipeline\n",
    "\n",
    "### 1. Load Output Data"
   ],
   "id": "9a5c95554b35fc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T19:46:36.498371Z",
     "start_time": "2025-03-22T19:46:36.354408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Output Labels from Dingle and Neiman\n",
    "labels = pd.read_csv('https://raw.githubusercontent.com/jdingel/DingelNeiman-workathome/master/occ_onet_scores/output/occupations_workathome.csv')\n",
    "labels.head()\n",
    "labels = labels['title'].to_numpy()\n"
   ],
   "id": "18d8be04cd40dd8b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  onetsoccode                                title  teleworkable\n",
       "0  11-1011.00                     Chief Executives             1\n",
       "1  11-1011.03        Chief Sustainability Officers             1\n",
       "2  11-1021.00      General and Operations Managers             1\n",
       "3  11-2011.00  Advertising and Promotions Managers             1\n",
       "4  11-2021.00                   Marketing Managers             1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onetsoccode</th>\n",
       "      <th>title</th>\n",
       "      <th>teleworkable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11-1011.00</td>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11-1011.03</td>\n",
       "      <td>Chief Sustainability Officers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-1021.00</td>\n",
       "      <td>General and Operations Managers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11-2011.00</td>\n",
       "      <td>Advertising and Promotions Managers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11-2021.00</td>\n",
       "      <td>Marketing Managers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Encode target classes\n",
    "\n",
    "Batch_size determines the number of observations to embed at once so to avoid memory issues. While higher batch sizes are faster, they may not fit in memory.\n",
    "\n"
   ],
   "id": "2a9cbb5a9ea50ca4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "encoder = Encoder(\"all-MiniLM-L6-v2\")\n",
    "y_labels, y_embeddings = encoder.embed(labels).save(\"embeds/occupations.npz\")"
   ],
   "id": "66988b7f86849ffc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Encode data\n",
    "\n",
    "Data here may be unlabeled (such as the Kaggle or Reddit data), but it does not say much for the model's performance. Consider creating labeled data for evaluation. For demonstration purposes, we create randomized data from the output labels by adding noise.\n",
    "\n",
    "You can optionally use a sample of the unlabelled data to see how well the model works!"
   ],
   "id": "4298b82e7197146a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use this block to use the synthetic data\n",
    "true, data = make_random_data(classes=labels, num_obs=10, colidx=1)\n",
    "x_labels, x_embeddings = encoder.embed(data, batched=True, batch_size=1024).save(\"embeds/test.npz\")"
   ],
   "id": "77b8387ce778bbb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use this block to use real labeled data\n",
    "num_obs = 100\n",
    "data = create_training_data(num_obs=num_obs) # samples 100 observations from the scraped data\n",
    "x_labels, x_embeddings = encoder.embed(data).save(\"embeds/test.npz\")"
   ],
   "id": "f4eb3eee4591f555"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Build Annoy Index for target classes\n",
    "The hyperparameter at training is the number of random trees to build. The more trees, the more accurate the search, but the longer it takes to build the index."
   ],
   "id": "a79992e8909c7b8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_trees = 500\n",
    "tree = Classifier().build_tree(y_labels, y_embeddings, num_trees=num_trees)"
   ],
   "id": "dbcafaeb1507d381"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Predict labels\n",
    "\n",
    "Below returns a tree with the retrieved labels and embeddings. Creating a separate tree objects allows for multiple predictions without rebuilding the index. The tree is self-contained enough to evaluate and visualize the data in separate instances.\n",
    "\n",
    "num_neighbors is the number of nearest neighbors to consider when classifying the data, equivalent to the k in KNN."
   ],
   "id": "ba7c63d171fa76a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_neighbors = 10\n",
    "predicted = tree.predict(x_labels, x_embeddings, neighbors=num_neighbors)"
   ],
   "id": "efdaadbc0298aebc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.5 (optional) Evaluate if using labeled data\n",
    "\n",
    "Returns the standard (micro) classification report from sklearn and calculated macro metrics. TMicro metrics include accuracy, precision, recall, f1 score, and support. Overall metrics are calculated by averaging the micro metrics across all classes.\n",
    "\n",
    "Only macro metrics are printed to the console, but the full report is saved to a csv file if a path is provided.\n",
    "\n"
   ],
   "id": "56720a190e661070"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "predicted.evaluate(true, save_path=\"eval_report\")",
   "id": "d0adedfbec9a52dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Visualize\n",
    "Create a 2d plot of the top 20 most common occupations in the sample. The plot shows the distribution of the embeddings in the feature space, and how they are clustered. Each point represents an observation, and the color represents the assigned class label. The X's represent the target classes."
   ],
   "id": "1ff8fa01f4344faf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# prints the top 20 occupations\n",
    "predicted.visualize(top=20, label_points=True, figsize=(10, 10), save=\"visualize.png\")"
   ],
   "id": "50e298fe9a72c46c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
