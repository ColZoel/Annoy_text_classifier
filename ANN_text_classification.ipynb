{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Approximate Nearest Neighbors for Text Classification\n",
    "Author: [Collin Zoeller](https://www.linkedin.com/in/collinzoeller)\n",
    "<br> Carnegie Mellon University\n",
    "\n",
    "This notebook demonstrates how to use the ANNOY library for fast approximate nearest neighbor search to classify text data. The goal is to classify user-generated text data into pre-defined categories using a pre-trained transformer model. The ANNOY library is used to build an approximate nearest neighbor index for the target classes, and then to classify new observations based on their nearest neighbors in the embedding space.\n"
   ],
   "id": "2e8cbd05f4483d2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Imports",
   "id": "6907ae99e42d90c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install -q sentence-transformers\n",
    "!pip install -q annoy\n",
    "!pip install -q kagglehub"
   ],
   "id": "9b43540e75b516a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T19:34:07.905440Z",
     "start_time": "2025-03-22T19:33:53.749965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from annoy import AnnoyIndex\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n"
   ],
   "id": "40ad25b6946a9cce",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/collinzoeller/anaconda3/envs/AnnoyClassifier/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "\n",
    "Use 100k samples of previously scraped data to train and evaluate the model. produce Synthetic data by adding noise to the scraped data.\n",
    "\n",
    "\n",
    "REDDIT DATA\n",
    "- Data retrieved from: https://www.reddit.com/r/datasets/comments/w340kj/dataset_of_job_descriptions_for_your_pleasure/\n",
    "- Data hosted at: https://drive.google.com/drive/folders/1XxNuhiei5taFR6gziofYAx0oWfGeV7y9\n",
    "\n",
    "KAGGLE DATA\n",
    "- SOURCE: https://www.kaggle.com/datasets/jatinchawda/job-titles-and-description\n"
   ],
   "id": "9bfb4c4dc0e04433"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_training_data(num_obs=None):\n",
    "    \"\"\"Create training data from scraped data.\"\"\"\n",
    "    df = pd.read_parquet('data/kaggle_clean_data.parquet')\n",
    "    df = df.rename(columns={\"job_title\": \"title\"})\n",
    "    df1 = pd.read_parquet('data/reddit_data.parquet')\n",
    "    df = pd.concat([df, df1])\n",
    "    df = clean_data(df)\n",
    "\n",
    "    if num_obs:\n",
    "        df = df.sample(num_obs)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_data(data:np.array):\n",
    "    \"\"\"\n",
    "    Standardize data for english ASCII-only characters.\n",
    "    :param data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data, columns=['X'])\n",
    "\n",
    "   # non-ascii\n",
    "    df['X'] = df['X'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "\n",
    "    # remove empty strings\n",
    "    df = df[df['X'] != \"\"]\n",
    "\n",
    "    return df.X.to_numpy()\n",
    "\n",
    "\n",
    "def noisify(truevals: np.array):\n",
    "    \"\"\"Add noise to the data.\"\"\"\n",
    "\n",
    "    df = pd.DataFrame(truevals, columns=['X'])\n",
    "    if random.random() < 0.3:\n",
    "        df['X'] = df['X'].str.upper()\n",
    "    if random.random() > 0.5:\n",
    "        df['X'] = df['X'].str.lower()\n",
    "    if random.random() < 0.5:\n",
    "        df['X'] = df['X'].apply(lambda x: x + \" \" + random.choice([\"Senior\", \"Junior\", \"Lead\"]))\n",
    "    if random.random() < 0.3:\n",
    "        df['X'] = df['X'].apply(lambda x: \"\".join(list(x).pop(random.randint(0, len(x) - 1))))\n",
    "    if random.random() < 0.1:\n",
    "        df['X'] = df['X'].str[::-1]\n",
    "    if random.random() < 0.2:\n",
    "        df['X'] = df['X'].apply(lambda x: x.replace(\" \", random.choice([\"_\", \"-\", \"\"])))\n",
    "    if random.random() < 0.2:\n",
    "        df['X'] = df['X'].apply(lambda x: x + str(random.randint(0, 99)))\n",
    "    if random.random() < 0.2:\n",
    "        df['X'] = df['X'].apply(lambda x: x[:random.randint(1, len(x))])\n",
    "    return df.X.to_numpy()\n",
    "\n",
    "\n",
    "def make_random_data(classes, num_obs=1000):\n",
    "    \"\"\"Create random data with noise.\"\"\"\n",
    "    true_values = np.random.choice(classes, num_obs)\n",
    "    noisy_data = noisify(true_values)\n",
    "    return true_values, noisy_data"
   ],
   "id": "d1dd9ddc2a433dd1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Download the Kaggle dataset",
   "id": "bb02cb78c80364cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if not os.path.exists(\"data/kaggle_clean_data.parquet\"):\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    path = kagglehub.dataset_download(\"jatinchawda/job-titles-and-description\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    shutil.move(f\"{path}/clean_data.parquet\", \"data/kaggle_clean_data.parquet\")\n",
    "\n",
    "    # Save only the title column\n",
    "    df = pd.read_parquet(\"data/kaggle_clean_data.parquet\", columns=[\"job_title\"])\n",
    "    df.to_parquet(\"data/kaggle_clean_data.parquet\")\n",
    "\n",
    "else:\n",
    "    print(\"Kaggle data already downloaded.\")"
   ],
   "id": "4be49ec29d468810"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Format the Reddit dataset\n",
    "This data should already be downloaded from https://drive.google.com/drive/folders/1XxNuhiei5taFR6gziofYAx0oWfGeV7y9 and saved as data/reddit_jobs."
   ],
   "id": "1e6068258b4a5f93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "if not os.path.exists(\"data/reddit_data.parquet\"):\n",
    "\n",
    "    if not os.path.exists(\"data/reddit_jobs\"):\n",
    "        raise FileNotFoundError(\"Download the Reddit dataset from the drive at\"\n",
    "                                \" https://drive.google.com/drive/folders/1XxNuhiei5taFR6gziofYAx0oWfGeV7y9 .\"\n",
    "                                \"\\nSave as data/reddit_jobs.\")\n",
    "\n",
    "    print(\"Formatting Reddit data...\")\n",
    "    df = pd.concat([pd.read_csv(file) for file in glob(f\"data/reddit_jobs/*.csv\")])\n",
    "    df=df['title'].to_frame()\n",
    "    df.to_parquet(\"data/reddit_data.parquet\")\n",
    "    print(\"Reddit data saved to data/reddit_data.parquet\")\n",
    "else:\n",
    "    print(\"Reddit data already formatted correctly.\")"
   ],
   "id": "949203c4e111f5a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "15d7cdd45d872b30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modules\n",
    "\n",
    "Functions and tools\n"
   ],
   "id": "25f0dcd99ae22bb7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions and tools for the occupation classification project.\n",
    "\"\"\"\n",
    "def embed_batched(model, data, batch_size=1000):\n",
    "    \"\"\"embeddings in batches.\"\"\"\n",
    "\n",
    "    num_batches = (len(data) + batch_size - 1) // batch_size\n",
    "    batch_indices = np.array_split(np.arange(len(data)), num_batches)\n",
    "\n",
    "    embeds = np.vstack([model.encode(data[indices], convert_to_numpy=True, show_progress_bar=False) for indices in batch_indices])\n",
    "\n",
    "    return embeds\n",
    "\n",
    "\n",
    "def build_tree(embeddings, num_trees=10):\n",
    "    \"\"\"Build an Annoy index for the given embeddings.\"\"\"\n",
    "    t = AnnoyIndex(embeddings.shape[1], 'euclidean')\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        t.add_item(i, emb)\n",
    "    t.build(num_trees)\n",
    "    print(\"Annoy index built with\", num_trees, \"trees.\")\n",
    "    return t\n",
    "\n",
    "\n",
    "def predict_labels(tree, classes, embeddings, neighbors=1):\n",
    "    \"\"\"Predict the nearest neighbor labels for the given embeddings.\"\"\"\n",
    "    # Retrieve neighbor indices for each embedding\n",
    "    indices = [tree.get_nns_by_vector(emb.tolist(), neighbors) for emb in embeddings]\n",
    "\n",
    "    # Map indices to their corresponding class labels\n",
    "    neighbor_labels = [[classes[idx] for idx in idxs] for idxs in indices]\n",
    "\n",
    "    # Determine the most common label among neighbors\n",
    "    most_common_labels = [Counter(labels).most_common(1)[0][0] for labels in neighbor_labels]\n",
    "\n",
    "    return np.array(most_common_labels)\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"Evaluate the classification performance.\"\"\"\n",
    "    comp = pd.DataFrame({\"label\": y_true, \"yhat\": y_pred})\n",
    "    comp['correct'] = comp['label'] == comp['yhat']\n",
    "    accuracy = comp['correct'].mean()\n",
    "    precision = comp.groupby('yhat')['correct'].mean().mean()\n",
    "    recall = comp.groupby('label')['correct'].mean().mean()\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\"\n",
    "          f\"\\nPrecision: {precision:.4f}\"\n",
    "          f\"\\nRecall: {recall:.4f}\"\n",
    "          f\"\\nF1: {f1:.4f}\")\n",
    "\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    pd.DataFrame(report).T.to_csv(\"report.csv\")\n",
    "    return\n",
    "\n",
    "\n",
    "def visualize(x_embeddings, x_labels, class_embeddings, class_labels, save: bool = False):\n",
    "    \"\"\"Visualize the embeddings using PCA.\"\"\"\n",
    "    x_embeddings = np.vstack(x_embeddings)\n",
    "    x_labels = np.array(x_labels)\n",
    "    class_embeddings = np.vstack(class_embeddings)\n",
    "    class_labels = np.array(class_labels)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_embed = pca.fit_transform(x_embeddings)\n",
    "    pca_class_embed = pca.transform(class_embeddings)\n",
    "\n",
    "    unique_classes, counts = np.unique(x_labels, return_counts=True)\n",
    "    top_20_indices = np.argsort(-counts)[:20]\n",
    "    top_5_indices = np.argsort(-counts)[:5]\n",
    "    unique_classes_20 = unique_classes[top_20_indices]\n",
    "    unique_classes_5 = unique_classes[top_5_indices]\n",
    "\n",
    "    colors = plt.colormaps['tab20']\n",
    "    class_to_color = {cls: colors(i) for i, cls in enumerate(unique_classes_20)}\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # plot input embeddings\n",
    "    for cls in unique_classes_20:\n",
    "        cls_idx = np.where(x_labels == cls)[0]\n",
    "        print(f\"Class: {cls}, Count: {len(cls_idx)}\")\n",
    "        plt.scatter(pca_embed[cls_idx, 0], pca_embed[cls_idx, 1],\n",
    "                    color=class_to_color[cls], alpha=0.6, marker='o')\n",
    "\n",
    "    # Label classes\n",
    "    for cls in unique_classes_20:\n",
    "        cls_idx = np.where(class_labels == cls)[0]\n",
    "        plt.scatter(pca_class_embed[cls_idx, 0], pca_class_embed[cls_idx, 1],\n",
    "                    color=class_to_color[cls], label=f\"{cls}\", alpha=1.0, marker='x', s=250)\n",
    "        # for idx in cls_idx:\n",
    "        #     plt.text(pca_class_embed[idx, 0], pca_class_embed[idx, 1], cls, fontsize=9, ha='center', va='bottom')\n",
    "\n",
    "    plt.title(\"Embedding Clusters in 2D: Top 20 Occupations\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.show()\n",
    "    if save:\n",
    "        plt.savefig(\"embeddings.png\")\n",
    "    return\n",
    "\n",
    "\n",
    "def pipeline(model: str, labels: np.array, data: np.array, num_trees: int, num_neighbors: int, batch_size: int, save_fig: bool = False):\n",
    "\n",
    "    # 1. Load pre-trained model\n",
    "    model = SentenceTransformer(model)\n",
    "\n",
    "    # 2. Encode target classes\n",
    "    enc_start = time.time()\n",
    "    print(f\"Encoding {len(labels)} target label values\")\n",
    "    target_embeddings = model.encode(labels, convert_to_numpy=True, show_progress_bar=True)\n",
    "    print(f\"(finished in {time.time() - enc_start:.2f} seconds, avg: {(time.time() - enc_start) / len(labels):.4f} sec/label)\")\n",
    "\n",
    "    # 3. Build Annoy Index for target classes\n",
    "    tree_start = time.time()\n",
    "    print(f\"Building Annoy index with {num_trees} trees\")\n",
    "    tree = build_tree(target_embeddings, num_trees=num_trees)\n",
    "    print(f\"(finished in {time.time() - tree_start:.2f} seconds, avg: {(time.time() - tree_start) / len(labels):.4f} sec/label)\")\n",
    "\n",
    "    # 4. Encode feature space and classify\n",
    "    features_start = time.time()\n",
    "    print(f\"Encoding {len(data)} feature vectors\")\n",
    "    feature_embeddings = embed_batched(model, data, batch_size=batch_size)\n",
    "    print(f\"(finished in {time.time() - features_start:.2f} seconds, avg: {(time.time() - features_start) / len(data):.4f} sec/label)\")\n",
    "\n",
    "    # 5. Predict labels\n",
    "    pred_start = time.time()\n",
    "    print(\"Predicting labels\")\n",
    "    yhat = predict_labels(tree, labels, feature_embeddings, neighbors=num_neighbors)\n",
    "    print(f\"(finished in {time.time() - pred_start:.2f} seconds, avg: {(time.time() - pred_start) / len(data):.4f} sec/label)\")\n",
    "\n",
    "    # 6. Visualize\n",
    "    visualize(feature_embeddings, yhat, target_embeddings, labels, save=save_fig)\n",
    "\n",
    "    return yhat\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pipeline\n",
    "\n",
    "### 1. Load pre-trained model and Output Data"
   ],
   "id": "9a5c95554b35fc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "modelname =\"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(modelname)"
   ],
   "id": "d550148551096d23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T19:46:36.498371Z",
     "start_time": "2025-03-22T19:46:36.354408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Output Labels from Dingle and Neiman\n",
    "labels = pd.read_csv('https://raw.githubusercontent.com/jdingel/DingelNeiman-workathome/master/occ_onet_scores/output/occupations_workathome.csv')\n",
    "labels.head()\n",
    "labels = labels['title'].to_numpy()"
   ],
   "id": "18d8be04cd40dd8b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  onetsoccode                                title  teleworkable\n",
       "0  11-1011.00                     Chief Executives             1\n",
       "1  11-1011.03        Chief Sustainability Officers             1\n",
       "2  11-1021.00      General and Operations Managers             1\n",
       "3  11-2011.00  Advertising and Promotions Managers             1\n",
       "4  11-2021.00                   Marketing Managers             1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onetsoccode</th>\n",
       "      <th>title</th>\n",
       "      <th>teleworkable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11-1011.00</td>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11-1011.03</td>\n",
       "      <td>Chief Sustainability Officers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-1021.00</td>\n",
       "      <td>General and Operations Managers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11-2011.00</td>\n",
       "      <td>Advertising and Promotions Managers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11-2021.00</td>\n",
       "      <td>Marketing Managers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Encode target classes",
   "id": "2a9cbb5a9ea50ca4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Encoding {len(labels)} target label values\")\n",
    "target_embeddings = model.encode(labels, convert_to_numpy=True, show_progress_bar=True)"
   ],
   "id": "66988b7f86849ffc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Build Annoy Index for target classes\n",
    "The hyperparameter at training is the number of random trees to build. The more trees, the more accurate the search, but the longer it takes to build the index."
   ],
   "id": "a79992e8909c7b8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_trees = 500\n",
    "tree = build_tree(target_embeddings, num_trees=num_trees)"
   ],
   "id": "dbcafaeb1507d381"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Encode feature space\n",
    "Batch_size determines the number of observations to embed at once so to avoid memory issues. While higher batch sizes are faster, they may not fit in memory.\n",
    "\n",
    "Data here may be unlabeled (such as the Kaggle or Reddit data), but it does not say much for the model's performance. Consider creating labeled data for evaluation. For demonstration purposes, we create randomized data from the output labels by adding noise."
   ],
   "id": "966eb3d37c3f0126"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Use this block to use real labeled data\n",
    "data = create_training_data(num_obs=100) # samples 100 observations from the scraped data"
   ],
   "id": "75a5902356bc644f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Assuming the data is labeled and saved to the csv file\n",
    "data = pd.read_csv(\"data/training_data.csv\")\n",
    "y = data['label'].to_numpy()"
   ],
   "id": "6b0e0e7fd187b6b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use this block to use the synthetic data\n",
    "y, data = make_random_data(labels, num_obs=100000)"
   ],
   "id": "14599fcf223efaf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "batch_size = 1024\n",
    "number_of_obs= 100000 # The number of observations to generate\n",
    "\n",
    "feature_embeddings = embed_batched(model, data, batch_size=batch_size)"
   ],
   "id": "1ef334f1b1048e09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Predict labels\n",
    "num_neighbors is the number of nearest neighbors to consider when classifying the data, equivalent to the k in KNN."
   ],
   "id": "ba7c63d171fa76a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_neighbors = 10\n",
    "yhat = predict_labels(tree, labels, feature_embeddings, neighbors=num_neighbors)"
   ],
   "id": "efdaadbc0298aebc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Visualize\n",
    "Create a 2d plot of the top 20 most common occupations in the sample. The plot shows the distribution of the embeddings in the feature space, and how they are clustered. Each point represents an observation, and the color represents the assigned class label. The X's represent the target classes."
   ],
   "id": "1ff8fa01f4344faf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# prints the top 20 occupations\n",
    "visualize(feature_embeddings, yhat, target_embeddings, labels, save=False)"
   ],
   "id": "50e298fe9a72c46c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  7. Evaluate",
   "id": "3cf810af001ae30c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "evaluate(y, yhat)",
   "id": "d7a1fe12a2adaa9e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
