{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Approximate Nearest Neighbors for Text Classification",
   "id": "43d67fcf94120f4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Author: [Collin Zoeller](www.linkedin.com/in/collinzoeller)\n",
    "<br> Carnegie Mellon University\n",
    "\n",
    "This notebook demonstrates how to use the ANNOY library for fast approximate nearest neighbor search to classify text data. The goal is to classify user-generated text data into pre-defined categories using a pre-trained transformer model. The ANNOY library is used to build an approximate nearest neighbor index for the target classes, and then to classify new observations based on their nearest neighbors in the embedding space.\n"
   ],
   "id": "2e8cbd05f4483d2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Imports",
   "id": "6907ae99e42d90c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from annoy import AnnoyIndex\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "id": "40ad25b6946a9cce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "(Need to collect wider breadth of data and host it on GitHub)"
   ],
   "id": "9bfb4c4dc0e04433"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modules\n",
    "\n",
    "Functions and tools\n"
   ],
   "id": "25f0dcd99ae22bb7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisify(occupation):\n",
    "    \"\"\"Introduce simple noise to a base occupation title.\"\"\"\n",
    "\n",
    "    # Occasionally append a random seniority level\n",
    "    if random.random() < 0.3:\n",
    "        occupation = occupation + \" \" + random.choice([\"Senior\", \"Junior\", \"Lead\"])\n",
    "\n",
    "    # missing letters\n",
    "    if random.random() < 0.3:\n",
    "        locc = list(occupation)\n",
    "        locc.pop(random.randint(0, len(occupation)-1))\n",
    "        occupation = \"\".join(locc)\n",
    "\n",
    "    # lowercase\n",
    "    if random.random() < 0.5:\n",
    "        occupation = occupation.lower()\n",
    "    # all caps\n",
    "    if random.random() < 0.1:\n",
    "        occupation = occupation.upper()\n",
    "\n",
    "    return occupation\n",
    "\n",
    "\n",
    "def make_random_data(onet_classes, num_obs=10000):\n",
    "    \"\"\"Generate synthetic data with noise.\"\"\"\n",
    "\n",
    "    obs = pd.DataFrame([random.choice(onet_classes) for _ in range(num_obs)], columns=[\"labels\"])\n",
    "    obs['x'] = obs['labels'].apply(noisify)\n",
    "    return obs.labels.to_numpy(), obs.x.to_numpy()\n",
    "\n",
    "\n",
    "def build_tree(embeddings, num_trees=10):\n",
    "    \"\"\"Build an Annoy index for the given embeddings.\"\"\"\n",
    "    # Initialize the Annoy index\n",
    "    t = AnnoyIndex(embeddings.shape[1], 'angular')\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        t.add_item(i, emb)\n",
    "    t.build(num_trees)\n",
    "    return t\n",
    "\n",
    "\n",
    "def embed_batched(model, data, dim, batch_size=1000):\n",
    "    \"\"\"embeddings in batches.\"\"\"\n",
    "\n",
    "    embeds = np.empty((0, dim))\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        batch_embeddings = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
    "        embeds = np.vstack([embeds, batch_embeddings])\n",
    "\n",
    "    return embeds\n",
    "\n",
    "\n",
    "def predict_labels(tree, classes, embeddings):\n",
    "    \"\"\"Predict the nearest neighbor labels for the given embeddings.\"\"\"\n",
    "    labels = np.array([])\n",
    "    for emb in embeddings:\n",
    "        idx = tree.get_nns_by_vector(emb, 1)[0]\n",
    "        labels = np.append(labels, classes[idx])\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"Evaluate the classification performance.\"\"\"\n",
    "    comp = pd.DataFrame({\"label\": y_true, \"yhat\": y_pred})\n",
    "    comp['correct'] = comp['label'] == comp['yhat']\n",
    "    accuracy = comp['correct'].mean()\n",
    "    precision = comp.groupby('yhat')['correct'].mean().mean()\n",
    "    recall = comp.groupby('label')['correct'].mean().mean()\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\"\n",
    "          f\"\\nPrecision: {precision:.4f}\"\n",
    "          f\"\\nRecall: {recall:.4f}\"\n",
    "          f\"\\nF1: {f1:.4f}\")\n",
    "\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    pd.DataFrame(report).T.to_csv(\"report.csv\")\n",
    "    return\n",
    "\n",
    "\n",
    "def visualize(embeddings, labels, save: bool = False):\n",
    "    \"\"\"Visualize the embeddings using PCA.\"\"\"\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_embed = pca.fit_transform(embeddings)\n",
    "    # pca_embed = np.column_stack((labels, pca_embed))\n",
    "    pca_embed[:, 1:] = pca_embed[:, 1:].astype(float)\n",
    "\n",
    "    unique_classes, counts = np.unique(labels, return_counts=True)\n",
    "    top_20_indices = np.argsort(-counts)[:20]\n",
    "    unique_classes = unique_classes[top_20_indices]\n",
    "\n",
    "    colors = plt.colormaps['tab20']\n",
    "    class_to_color = {cls: colors(i) for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        cls_idx = np.where(labels == cls)[0]\n",
    "\n",
    "        plt.scatter(pca_embed[cls_idx, 0], pca_embed[cls_idx, 1],\n",
    "                    color=class_to_color[cls], label=cls, alpha=0.5)\n",
    "\n",
    "    plt.title(\"Embedding Clusters in 2D: Top 20 Occupations\")\n",
    "    plt.show()\n",
    "    if save:\n",
    "        plt.savefig(\"embeddings.png\")\n",
    "    return\n",
    "\n",
    "\n",
    "def pipeline(model: str, labels: np.array, data: np.array, num_trees: int, batch_size: int, save_fig: bool = False):\n",
    "\n",
    "    # 1. Load pre-trained model\n",
    "    model = SentenceTransformer(model)\n",
    "\n",
    "    # 2. Encode target classes\n",
    "    print(f\"Encoding {len(labels)} target label values\")\n",
    "    target_embeddings = model.encode(labels, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "    # 3. Build Annoy Index for target classes\n",
    "    print(f\"Building Annoy index with {num_trees} trees\")\n",
    "    tree = build_tree(target_embeddings, num_trees=num_trees)\n",
    "\n",
    "    # 4. Encode feature space and classify\n",
    "    print(f\"Encoding {len(data)} feature vectors\")\n",
    "    feature_embeddings = embed_batched(model, data, target_embeddings.shape[1], batch_size=batch_size)\n",
    "\n",
    "    # 5. Predict labels\n",
    "    print(\"Predicting labels\")\n",
    "    yhat = predict_labels(tree, labels, feature_embeddings)\n",
    "\n",
    "    # 6. Visualize\n",
    "    visualize(feature_embeddings, yhat, save=save_fig)\n",
    "\n",
    "    return yhat"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pipeline\n",
    "\n",
    "### 1. Load pre-trained model"
   ],
   "id": "9a5c95554b35fc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "modelname =\"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(modelname)"
   ],
   "id": "d550148551096d23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Encode target classes",
   "id": "2a9cbb5a9ea50ca4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Encoding {len(labels)} target label values\")\n",
    "target_embeddings = model.encode(labels, convert_to_numpy=True, show_progress_bar=True)"
   ],
   "id": "66988b7f86849ffc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
